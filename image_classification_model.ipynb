{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "375e221f",
   "metadata": {},
   "source": [
    "Importing the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "723402d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854da40f",
   "metadata": {},
   "source": [
    "Loading the CIFAR-10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "04b6da33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device will determine whether to run the training on GPU or CPU.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f7e5eb23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset cifar10 (C:/Users/rajes/.cache/huggingface/datasets/cifar10/plain_text/1.0.0/447d6ec4733dddd1ce3bb577c7166b986eaa4c538dcd9e805ba61f35674a9de4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['img', 'label'],\n",
       "    num_rows: 50000\n",
       "})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_train = load_dataset(\n",
    "    'cifar10',\n",
    "    split='train', # training dataset\n",
    "    ignore_verifications=True  # set to True if seeing splits Error\n",
    ")\n",
    "\n",
    "dataset_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6110cf0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking the number of classes\n",
    "num_classes = len(set(dataset_train['label']))\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2859ed30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset cifar10 (C:/Users/rajes/.cache/huggingface/datasets/cifar10/plain_text/1.0.0/447d6ec4733dddd1ce3bb577c7166b986eaa4c538dcd9e805ba61f35674a9de4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['img', 'label'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training the model\n",
    "dataset_val = load_dataset(\n",
    "    'cifar10',\n",
    "    split='test', # test dataset\n",
    "    ignore_verifications=True  # set to True if seeing splits Error\n",
    ")\n",
    "\n",
    "dataset_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5e35b388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image size\n",
    "img_size = 32\n",
    "\n",
    "# setting the preprocessor variable\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((img_size,img_size)),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "edcf3c1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba57cfc651db42bc9fd1d09634c0f11f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "inputs_train = []\n",
    "\n",
    "for record in tqdm(dataset_train):\n",
    "    image = record['img']\n",
    "    label = record['label']\n",
    "\n",
    "    # convert from grayscale to RGB\n",
    "    if image.mode == 'L':\n",
    "        image = image.convert(\"RGB\")\n",
    "        \n",
    "    # prepocessing\n",
    "    input_tensor = preprocess(image)\n",
    "    \n",
    "    # append to batch list\n",
    "    inputs_train.append([input_tensor, label]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "977f245a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000 torch.Size([3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "print(len(inputs_train), inputs_train[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d44d0470",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[0.6980, 0.6980, 0.6980,  ..., 0.6667, 0.6588, 0.6471],\n",
       "          [0.7059, 0.7020, 0.7059,  ..., 0.6784, 0.6706, 0.6588],\n",
       "          [0.6941, 0.6941, 0.6980,  ..., 0.6706, 0.6627, 0.6549],\n",
       "          ...,\n",
       "          [0.4392, 0.4431, 0.4471,  ..., 0.3922, 0.3843, 0.3961],\n",
       "          [0.4392, 0.4392, 0.4431,  ..., 0.4000, 0.4000, 0.4000],\n",
       "          [0.4039, 0.3922, 0.4039,  ..., 0.3608, 0.3647, 0.3569]],\n",
       " \n",
       "         [[0.6902, 0.6902, 0.6902,  ..., 0.6588, 0.6510, 0.6392],\n",
       "          [0.6980, 0.6941, 0.6980,  ..., 0.6706, 0.6627, 0.6510],\n",
       "          [0.6863, 0.6863, 0.6902,  ..., 0.6627, 0.6549, 0.6471],\n",
       "          ...,\n",
       "          [0.4196, 0.4275, 0.4314,  ..., 0.3804, 0.3686, 0.3725],\n",
       "          [0.4000, 0.4039, 0.4039,  ..., 0.3725, 0.3647, 0.3608],\n",
       "          [0.3765, 0.3647, 0.3725,  ..., 0.3294, 0.3373, 0.3294]],\n",
       " \n",
       "         [[0.7412, 0.7412, 0.7412,  ..., 0.7059, 0.6941, 0.6824],\n",
       "          [0.7490, 0.7451, 0.7490,  ..., 0.7137, 0.7059, 0.6941],\n",
       "          [0.7373, 0.7373, 0.7412,  ..., 0.7059, 0.6980, 0.6902],\n",
       "          ...,\n",
       "          [0.4196, 0.4235, 0.4314,  ..., 0.3686, 0.3647, 0.3725],\n",
       "          [0.3961, 0.4000, 0.4039,  ..., 0.3647, 0.3569, 0.3569],\n",
       "          [0.3608, 0.3529, 0.3686,  ..., 0.3137, 0.3137, 0.3020]]]),\n",
       " 0]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_train[0] #checking whether all values are normalized between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "90a61adc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512,)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)#setting seed to confirm that the validation set is always used\n",
    "\n",
    "idx = np.random.randint(0, len(inputs_train), 512)\n",
    "idx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e6a0ca44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 16384, 32])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# concatenating the image tensors\n",
    "tensors = torch.concat([inputs_train[i][0] for i in idx], axis=1)\n",
    "tensors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7e57464c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([524288, 3])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merging values\n",
    "tensors = tensors.swapaxes(0, 1).reshape(3, -1).T\n",
    "tensors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a052c4a",
   "metadata": {},
   "source": [
    "Calculating mean and standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "85e81865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4670, 0.4735, 0.4662])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean = torch.mean(tensors, axis=0)\n",
    "mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8e6691b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2496, 0.2489, 0.2521])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std = torch.std(tensors, axis=0)\n",
    "std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bb932f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "del tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ad7e8b40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9c98ea0de554f8fb5b137c368b27344",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Normalizing the tensors\n",
    "preprocess = transforms.Compose([transforms.Normalize(mean=mean, std=std)])\n",
    "\n",
    "for i in tqdm(range(len(inputs_train))):\n",
    "    # prepocessing\n",
    "    input_tensor = preprocess(inputs_train[i][0])\n",
    "    inputs_train[i][0] = input_tensor  # replace with normalized tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ca361099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging the two preprocessing steps from before\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((img_size,img_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "77d385fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99392334bccc4dfeb76cf0906db8ca1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "inputs_val = []\n",
    "i = 0\n",
    "for record in tqdm(dataset_val):\n",
    "    image = record['img']\n",
    "    label = record['label']\n",
    "\n",
    "    # convert from grayscale to RBG\n",
    "    if image.mode == 'L':\n",
    "        image = image.convert(\"RGB\")\n",
    "        \n",
    "    # prepocessing\n",
    "    input_tensor = preprocess(image)\n",
    "    inputs_val.append((input_tensor, label)) # append to batch list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855d3190",
   "metadata": {},
   "source": [
    "To increase efficiency, I'm dividing the images into batches for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "82d1f20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define batch size\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "11e54219",
   "metadata": {},
   "outputs": [],
   "source": [
    "dloader_train = torch.utils.data.DataLoader(\n",
    "    inputs_train, batch_size=batch_size, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "01c845e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dloader_val = torch.utils.data.DataLoader(\n",
    "    inputs_val, batch_size=batch_size, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659d4468",
   "metadata": {},
   "source": [
    "Now to build the Neural Network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ecfa6ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a CNN class\n",
    "class ConvNeuralNet(nn.Module):\n",
    "\t#  determine what layers and their order in CNN object \n",
    "    def __init__(self, num_classes):\n",
    "        super(ConvNeuralNet, self).__init__()\n",
    "        self.conv_layer1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=4, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.max_pool1 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "\n",
    "        self.conv_layer2 = nn.Conv2d(in_channels=64, out_channels=192, kernel_size=4, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.max_pool2 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "\n",
    "        self.conv_layer3 = nn.Conv2d(in_channels=192, out_channels=384, kernel_size=3, padding=1)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        \n",
    "        self.conv_layer4 = nn.Conv2d(in_channels=384, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.relu4 = nn.ReLU()\n",
    "\n",
    "        self.conv_layer5 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.relu5 = nn.ReLU()\n",
    "        self.max_pool5 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        \n",
    "        self.dropout6 = nn.Dropout(p=0.5)\n",
    "        self.fc6 = nn.Linear(1024, 512)\n",
    "        self.relu6 = nn.ReLU()\n",
    "        self.dropout7 = nn.Dropout(p=0.5)\n",
    "        self.fc7 = nn.Linear(512, 256)\n",
    "        self.relu7 = nn.ReLU()\n",
    "        self.fc8 = nn.Linear(256, num_classes)\n",
    "    \n",
    "    # progresses data across layers    \n",
    "    def forward(self, x):\n",
    "        out = self.conv_layer1(x)\n",
    "        out = self.relu1(out)\n",
    "        out = self.max_pool1(out)\n",
    "        \n",
    "        out = self.conv_layer2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.max_pool2(out)\n",
    "\n",
    "        out = self.conv_layer3(out)\n",
    "        out = self.relu3(out)\n",
    "\n",
    "        out = self.conv_layer4(out)\n",
    "        out = self.relu4(out)\n",
    "\n",
    "        out = self.conv_layer5(out)\n",
    "        out = self.relu5(out)\n",
    "        out = self.max_pool5(out)\n",
    "        \n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        \n",
    "        out = self.dropout6(out)\n",
    "        out = self.fc6(out)\n",
    "        out = self.relu6(out)\n",
    "\n",
    "        out = self.dropout7(out)\n",
    "        out = self.fc7(out)\n",
    "        out = self.relu7(out)\n",
    "\n",
    "        out = self.fc8(out)  # final logits\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f2562fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the model for training\n",
    "model = ConvNeuralNet(num_classes).to(device)\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "# set learning rate \n",
    "lr = 0.1\n",
    "\n",
    "# setting optimizer as SGD \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c8470364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], train_loss: 1.8053, val-loss: 1.8845, val-acc: 24.6%\n",
      "Epoch [2/20], train_loss: 1.5106, val-loss: 1.5195, val-acc: 42.7%\n",
      "Epoch [3/20], train_loss: 1.0853, val-loss: 2.3137, val-acc: 37.5%\n",
      "Epoch [4/20], train_loss: 0.9178, val-loss: 1.1954, val-acc: 59.0%\n",
      "Epoch [5/20], train_loss: 0.7322, val-loss: 0.9406, val-acc: 67.7%\n",
      "Epoch [6/20], train_loss: 1.3147, val-loss: 1.0267, val-acc: 66.3%\n",
      "Epoch [7/20], train_loss: 0.3104, val-loss: 0.8386, val-acc: 72.5%\n",
      "Epoch [8/20], train_loss: 0.3476, val-loss: 0.9990, val-acc: 70.1%\n",
      "Epoch [9/20], train_loss: 0.5926, val-loss: 1.0962, val-acc: 68.7%\n",
      "Epoch [10/20], train_loss: 0.3866, val-loss: 0.8438, val-acc: 75.7%\n",
      "Epoch [11/20], train_loss: 0.3246, val-loss: 0.7067, val-acc: 78.5%\n",
      "Epoch [12/20], train_loss: 0.3917, val-loss: 1.0653, val-acc: 70.1%\n",
      "Epoch [13/20], train_loss: 0.1026, val-loss: 0.8092, val-acc: 78.9%\n",
      "Epoch [14/20], train_loss: 0.0129, val-loss: 0.7691, val-acc: 79.7%\n",
      "Epoch [15/20], train_loss: 0.2551, val-loss: 0.9009, val-acc: 77.4%\n",
      "Epoch [16/20], train_loss: 0.3790, val-loss: 1.8005, val-acc: 68.2%\n",
      "Epoch [17/20], train_loss: 0.0302, val-loss: 0.8064, val-acc: 80.3%\n",
      "Epoch [18/20], train_loss: 0.2526, val-loss: 0.8349, val-acc: 80.0%\n",
      "Epoch [19/20], train_loss: 0.0275, val-loss: 0.9417, val-acc: 80.3%\n",
      "Epoch [20/20], train_loss: 0.0073, val-loss: 0.9227, val-acc: 81.1%\n"
     ]
    }
   ],
   "source": [
    "# training and validating the network\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "\t# loading in the data in batches\n",
    "    for i, (images, labels) in enumerate(dloader_train):  \n",
    "        # move tensors to the configured device\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # forward propagation\n",
    "        outputs = model(images)\n",
    "        loss = loss_func(outputs, labels)\n",
    "        \n",
    "        # backward propagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    # at end of epoch I'm checking validation loss and accuracy on validation set\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        all_val_loss = []\n",
    "        for images, labels in dloader_val:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            total += labels.size(0)\n",
    "            # calculate predictions\n",
    "            predicted = torch.argmax(outputs, dim=1)\n",
    "            # calculate actual values\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            # calculate the loss\n",
    "            all_val_loss.append(loss_func(outputs, labels).item())\n",
    "        # calculate val-loss\n",
    "        mean_val_loss = sum(all_val_loss) / len(all_val_loss)\n",
    "        # calculate val-accuracy\n",
    "        mean_val_acc = 100 * (correct / total)\n",
    "\n",
    "    print(\n",
    "        'Epoch [{}/{}], train_loss: {:.4f}, val-loss: {:.4f}, val-acc: {:.1f}%'.format(\n",
    "            epoch+1, num_epochs, loss.item(), mean_val_loss, mean_val_acc\n",
    "        )\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
